/*
 * Licensed to Elasticsearch under one or more contributor
 * license agreements. See the NOTICE file distributed with
 * this work for additional information regarding copyright
 * ownership. Elasticsearch licenses this file to you under
 * the Apache License, Version 2.0 (the "License"); you may
 * not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing,
 * software distributed under the License is distributed on an
 * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
 * KIND, either express or implied.  See the License for the
 * specific language governing permissions and limitations
 * under the License.
 */

//apply plugin: 'nebula.provided-base'

import org.apache.tools.ant.taskdefs.condition.Os
import java.nio.file.Files
import java.nio.file.Path
import java.nio.file.Paths
 
esplugin {
  description 'The HDFS repository plugin adds support for Hadoop Distributed File-System (HDFS) repositories.'
  classname 'org.elasticsearch.repositories.hdfs.HdfsPlugin'
}

versions << [
  'hadoop2': '2.7.1'
]

configurations {
  hdfsFixture
}

dependencies {
  compile "org.apache.hadoop:hadoop-client:${versions.hadoop2}"
  compile "org.apache.hadoop:hadoop-common:${versions.hadoop2}"
  compile "org.apache.hadoop:hadoop-annotations:${versions.hadoop2}"
  compile "org.apache.hadoop:hadoop-auth:${versions.hadoop2}"
  compile "org.apache.hadoop:hadoop-hdfs:${versions.hadoop2}"
  compile 'org.apache.htrace:htrace-core:3.1.0-incubating'
  compile 'com.google.guava:guava:16.0.1'
  compile 'com.google.protobuf:protobuf-java:2.5.0'
  compile 'commons-logging:commons-logging:1.1.3'
  compile 'commons-collections:commons-collections:3.2.2'
  compile 'commons-configuration:commons-configuration:1.6'
  compile 'commons-io:commons-io:2.4'
  compile 'commons-lang:commons-lang:2.6'
  compile 'javax.servlet:servlet-api:2.5'
  compile "org.slf4j:slf4j-api:${versions.slf4j}"

  hdfsFixture project(':test:fixtures:hdfs-fixture')
}

dependencyLicenses {
  mapping from: /hadoop-.*/, to: 'hadoop'
}

task hdfsFixture(type: org.elasticsearch.gradle.test.Fixture) {
  dependsOn project.configurations.hdfsFixture
  executable = new File(project.javaHome, 'bin/java')
  env 'CLASSPATH', "${ -> project.configurations.hdfsFixture.asPath }"
  args 'hdfs.MiniHDFS',
       baseDir
}

integTest {
  boolean fixtureSupported = false;
  if (Os.isFamily(Os.FAMILY_WINDOWS)) {
    // hdfs fixture will not start without hadoop native libraries on windows
    String nativePath = System.getenv("HADOOP_HOME")
    if (nativePath != null) {
      Path path = Paths.get(nativePath);
      if (Files.isDirectory(path) &&
          Files.exists(path.resolve("bin").resolve("winutils.exe")) &&
          Files.exists(path.resolve("bin").resolve("hadoop.dll")) &&
          Files.exists(path.resolve("bin").resolve("hdfs.dll"))) {
        fixtureSupported = true
      } else {
        throw new IllegalStateException("HADOOP_HOME: " + path.toString() + " is invalid, does not contain hadoop native libraries in $HADOOP_HOME/bin");
      }
    }
  } else {
    fixtureSupported = true
  }

  if (fixtureSupported) {
    dependsOn hdfsFixture
  } else {
    logger.warn("hdfsFixture unsupported, please set HADOOP_HOME and put HADOOP_HOME\\bin in PATH")
    // just tests that the plugin loads
    systemProperty 'tests.rest.suite', 'hdfs_repository/10_basic'
  }
}

compileJava.options.compilerArgs << '-Xlint:-deprecation,-rawtypes'

thirdPartyAudit.excludes = [
  // classes are missing
  'javax.mail.internet.MimeMultipart', 
  'javax.mail.util.ByteArrayDataSource', 
  'org.apache.commons.beanutils.BeanUtils', 
  'org.apache.commons.beanutils.DynaBean', 
  'org.apache.commons.beanutils.DynaClass', 
  'org.apache.commons.beanutils.DynaProperty', 
  'org.apache.commons.beanutils.PropertyUtils', 
  'org.apache.commons.digester.AbstractObjectCreationFactory', 
  'org.apache.commons.digester.CallMethodRule', 
  'org.apache.commons.digester.Digester', 
  'org.apache.commons.digester.ObjectCreationFactory', 
  'org.apache.commons.digester.substitution.MultiVariableExpander', 
  'org.apache.commons.digester.substitution.VariableSubstitutor', 
  'org.apache.commons.digester.xmlrules.DigesterLoader', 
  'org.apache.commons.httpclient.Header', 
  'org.apache.commons.httpclient.HttpClient', 
  'org.apache.commons.httpclient.methods.GetMethod', 
  'org.apache.commons.jxpath.JXPathContext', 
  'org.apache.commons.jxpath.ri.JXPathContextReferenceImpl', 
  'org.apache.commons.jxpath.ri.QName', 
  'org.apache.commons.jxpath.ri.compiler.NodeNameTest', 
  'org.apache.commons.jxpath.ri.compiler.NodeTest', 
  'org.apache.commons.jxpath.ri.compiler.NodeTypeTest', 
  'org.apache.commons.jxpath.ri.model.NodeIterator', 
  'org.apache.commons.jxpath.ri.model.NodePointer', 
  'org.apache.commons.jxpath.ri.model.NodePointerFactory', 
  'org.apache.ftpserver.DefaultFtpServerContext', 
  'org.apache.ftpserver.FtpServer', 
  'org.apache.ftpserver.ftplet.Authority', 
  'org.apache.ftpserver.ftplet.UserManager', 
  'org.apache.ftpserver.interfaces.FtpServerContext', 
  'org.apache.ftpserver.listener.mina.MinaListener', 
  'org.apache.ftpserver.usermanager.BaseUser', 
  'org.apache.ftpserver.usermanager.WritePermission', 
  'org.apache.hadoop.examples.DBCountPageView', 
  'org.apache.hadoop.examples.PiEstimator', 
  'org.apache.hadoop.examples.RandomWriter', 
  'org.apache.hadoop.examples.SecondarySort$FirstGroupingComparator', 
  'org.apache.hadoop.examples.SecondarySort$FirstPartitioner', 
  'org.apache.hadoop.examples.SecondarySort$IntPair', 
  'org.apache.hadoop.examples.SecondarySort$MapClass', 
  'org.apache.hadoop.examples.SecondarySort$Reduce', 
  'org.apache.hadoop.examples.SleepJob$SleepInputFormat', 
  'org.apache.hadoop.examples.SleepJob', 
  'org.apache.hadoop.examples.Sort', 
  'org.apache.hadoop.examples.WordCount$IntSumReducer', 
  'org.apache.hadoop.examples.WordCount$TokenizerMapper', 
  'org.apache.hadoop.examples.WordCount', 
  'org.apache.hadoop.tools.DistCh', 
  'org.apache.hadoop.tools.DistCp', 
  'org.apache.hadoop.tools.HadoopArchives', 
  'org.apache.hadoop.tools.distcp2.CopyListing', 
  'org.apache.hadoop.tools.distcp2.DistCp', 
  'org.apache.hadoop.tools.distcp2.DistCpOptionSwitch', 
  'org.apache.hadoop.tools.distcp2.DistCpOptions$FileAttribute', 
  'org.apache.hadoop.tools.distcp2.DistCpOptions', 
  'org.apache.hadoop.tools.distcp2.FileBasedCopyListing', 
  'org.apache.hadoop.tools.distcp2.GlobbedCopyListing', 
  'org.apache.hadoop.tools.distcp2.OptionsParser', 
  'org.apache.hadoop.tools.distcp2.SimpleCopyListing', 
  'org.apache.hadoop.tools.distcp2.mapred.CopyCommitter', 
  'org.apache.hadoop.tools.distcp2.mapred.CopyMapper$Counter', 
  'org.apache.hadoop.tools.distcp2.mapred.CopyMapper', 
  'org.apache.hadoop.tools.distcp2.mapred.CopyOutputFormat', 
  'org.apache.hadoop.tools.distcp2.mapred.UniformSizeInputFormat', 
  'org.apache.hadoop.tools.distcp2.mapred.lib.DynamicInputFormat', 
  'org.apache.hadoop.tools.distcp2.util.DistCpUtils', 
  'org.apache.hadoop.tools.distcp2.util.RetriableCommand', 
  'org.apache.hadoop.tools.distcp2.util.ThrottledInputStream', 
  'org.apache.hadoop.tools.rumen.CDFPiecewiseLinearRandomGenerator', 
  'org.apache.hadoop.tools.rumen.CDFRandomGenerator', 
  'org.apache.hadoop.tools.rumen.DeepCompare', 
  'org.apache.hadoop.tools.rumen.DeepInequalityException', 
  'org.apache.hadoop.tools.rumen.DefaultInputDemuxer', 
  'org.apache.hadoop.tools.rumen.EventType', 
  'org.apache.hadoop.tools.rumen.HadoopLogsAnalyzer', 
  'org.apache.hadoop.tools.rumen.Histogram', 
  'org.apache.hadoop.tools.rumen.HistoryEvent', 
  'org.apache.hadoop.tools.rumen.InputDemuxer', 
  'org.apache.hadoop.tools.rumen.JobBuilder', 
  'org.apache.hadoop.tools.rumen.JobConfPropertyNames', 
  'org.apache.hadoop.tools.rumen.JobConfigurationParser', 
  'org.apache.hadoop.tools.rumen.JobHistoryParser', 
  'org.apache.hadoop.tools.rumen.JobHistoryParserFactory', 
  'org.apache.hadoop.tools.rumen.JobStory', 
  'org.apache.hadoop.tools.rumen.JobTraceReader', 
  'org.apache.hadoop.tools.rumen.JsonObjectMapperParser', 
  'org.apache.hadoop.tools.rumen.LogRecordType', 
  'org.apache.hadoop.tools.rumen.LoggedDiscreteCDF', 
  'org.apache.hadoop.tools.rumen.LoggedJob', 
  'org.apache.hadoop.tools.rumen.LoggedNetworkTopology', 
  'org.apache.hadoop.tools.rumen.LoggedSingleRelativeRanking', 
  'org.apache.hadoop.tools.rumen.LoggedTask', 
  'org.apache.hadoop.tools.rumen.LoggedTaskAttempt', 
  'org.apache.hadoop.tools.rumen.Pair', 
  'org.apache.hadoop.tools.rumen.ParsedJob', 
  'org.apache.hadoop.tools.rumen.ParsedLine', 
  'org.apache.hadoop.tools.rumen.ParsedTask', 
  'org.apache.hadoop.tools.rumen.ParsedTaskAttempt', 
  'org.apache.hadoop.tools.rumen.PossiblyDecompressedInputStream', 
  'org.apache.hadoop.tools.rumen.Pre21JobHistoryConstants$Values', 
  'org.apache.hadoop.tools.rumen.RandomSeedGenerator', 
  'org.apache.hadoop.tools.rumen.ResourceUsageMetrics', 
  'org.apache.hadoop.tools.rumen.RewindableInputStream', 
  'org.apache.hadoop.tools.rumen.TaskAttemptFinishedEvent', 
  'org.apache.hadoop.tools.rumen.TaskAttemptInfo', 
  'org.apache.hadoop.tools.rumen.TaskAttemptUnsuccessfulCompletionEvent', 
  'org.apache.hadoop.tools.rumen.TaskInfo', 
  'org.apache.hadoop.tools.rumen.TaskStartedEvent', 
  'org.apache.hadoop.tools.rumen.TopologyBuilder', 
  'org.apache.hadoop.tools.rumen.TraceBuilder$MyOptions', 
  'org.apache.hadoop.tools.rumen.TraceBuilder', 
  'org.apache.hadoop.tools.rumen.TreePath', 
  'org.apache.hadoop.tools.rumen.ZombieCluster', 
  'org.apache.hadoop.tools.rumen.ZombieJob', 
  'org.apache.hadoop.tools.rumen.ZombieJobProducer', 
  'org.apache.oro.text.regex.MatchResult', 
  'org.apache.oro.text.regex.Pattern', 
  'org.apache.oro.text.regex.PatternMatcher', 
  'org.apache.oro.text.regex.Perl5Compiler', 
  'org.apache.oro.text.regex.Perl5Matcher', 
  'org.apache.tools.ant.taskdefs.Execute', 
  'org.codehaus.jackson.JsonEncoding', 
  'org.codehaus.jackson.JsonFactory', 
  'org.codehaus.jackson.JsonGenerator', 
  'org.codehaus.jackson.map.ObjectMapper', 
  'org.hsqldb.Server', 
  'org.mockito.AdditionalMatchers', 
  'org.mockito.ArgumentCaptor', 
  'org.mockito.Matchers', 
  'org.mockito.Mockito', 
  'org.mockito.invocation.InvocationOnMock', 
  'org.mockito.stubbing.Answer', 
  'org.mockito.stubbing.OngoingStubbing', 
  'org.mockito.stubbing.Stubber', 
  'org.osgi.framework.Bundle', 
  'org.osgi.framework.BundleActivator', 
  'org.osgi.framework.BundleContext', 
  'org.osgi.framework.BundleEvent', 
  'org.osgi.framework.SynchronousBundleListener',

  // note: the jersey ones may be bogus, see my bug report at forbidden-apis!
  // internal java api: com.sun.jersey.server.impl.inject.AbstractHttpContextInjectable
  // internal java api: com.sun.jersey.api.core.HttpContext
  // internal java api: com.sun.jersey.core.spi.component.ComponentScope
  // internal java api: com.sun.jersey.spi.inject.Injectable
  // internal java api: com.sun.jersey.core.spi.component.ComponentContext
  'org.apache.hadoop.hdfs.web.resources.UserProvider',

  // internal java api: com.sun.jersey.spi.container.ResourceFilters
  'org.apache.hadoop.hdfs.server.namenode.web.resources.NamenodeWebHdfsMethods',
  // internal java api: com.sun.jersey.spi.container.servlet.ServletContainer
  'org.apache.hadoop.http.HttpServer', 
  'org.apache.hadoop.http.HttpServer2',

  // internal java api: com.sun.jersey.api.ParamException
  'org.apache.hadoop.hdfs.web.resources.ExceptionHandler',
  'org.apache.hadoop.hdfs.server.datanode.web.webhdfs.ExceptionHandler',
  'org.apache.hadoop.hdfs.web.ParamFilter',

  // internal java api: com.sun.jersey.spi.container.ContainerRequestFilter
  // internal java api: com.sun.jersey.spi.container.ContainerRequest
  'org.apache.hadoop.hdfs.web.ParamFilter',
  'org.apache.hadoop.hdfs.web.ParamFilter$1',

  // internal java api: com.sun.jndi.ldap.LdapCtxFactory
  'org.apache.hadoop.security.LdapGroupsMapping',

  // internal java api: sun.net.dns.ResolverConfiguration
  // internal java api: sun.net.util.IPAddressUtil
  'org.apache.hadoop.security.SecurityUtil$QualifiedHostResolver',

  // internal java api: sun.misc.Unsafe
  'com.google.common.cache.Striped64',
  'com.google.common.cache.Striped64$1',
  'com.google.common.cache.Striped64$Cell',
  'com.google.common.primitives.UnsignedBytes$LexicographicalComparatorHolder$UnsafeComparator', 
  'com.google.common.primitives.UnsignedBytes$LexicographicalComparatorHolder$UnsafeComparator$1',
  'org.apache.hadoop.io.FastByteComparisons$LexicographicalComparerHolder$UnsafeComparer',
  'org.apache.hadoop.io.FastByteComparisons$LexicographicalComparerHolder$UnsafeComparer$1',
  'org.apache.hadoop.io.nativeio.NativeIO',
  'org.apache.hadoop.hdfs.shortcircuit.ShortCircuitShm',
  'org.apache.hadoop.hdfs.shortcircuit.ShortCircuitShm$Slot',

  // internal java api: sun.nio.ch.DirectBuffer
  // internal java api: sun.misc.Cleaner
  'org.apache.hadoop.io.nativeio.NativeIO$POSIX',
  'org.apache.hadoop.crypto.CryptoStreamUtils',
 
  // internal java api: sun.misc.SignalHandler
  'org.apache.hadoop.util.SignalLogger$Handler',
]
